import os
import random
import cv2
import numpy as np
from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip, vfx
from moviepy.audio.io.AudioFileClip import AudioFileClip

class VideoEditorService:
    def __init__(self, output_dir="uploads"):
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    @staticmethod
    def _make_vignette(h, w, strength=0.35):
        """–°–æ–∑–¥–∞–µ—Ç –º–∞—Å–∫—É –≤–∏–Ω—å–µ—Ç–∫–∏ (—Ç—ë–º–Ω—ã–µ —É–≥–ª—ã)"""
        Y, X = np.ogrid[:h, :w]
        dist = np.sqrt(((X - w/2) / (w/2))**2 + ((Y - h/2) / (h/2))**2)
        return np.clip(1 - dist * strength, 0.45, 1.0)

    def _apply_filter(self, clip, filter_name):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –∫ –≤–∏–¥–µ–æ"""
        print(f"üé® [VideoEditor] Applying filter: '{filter_name}'")

        if not filter_name or filter_name == "No Filter":
            return clip

        # 1. BLACK & WHITE ‚Äî –∑–µ—Ä–Ω–∏—Å—Ç–æ—Å—Ç—å + –≤–∏–Ω—å–µ—Ç–∫–∞ + –∫–æ–Ω—Ç—Ä–∞—Å—Ç
        if filter_name == "Black & White":
            print("   -> Applying BW + Film Grain")
            bw_clip = clip.fx(vfx.blackwhite)
            w_v, h_v = clip.size
            vignette = self._make_vignette(h_v, w_v, 0.4)

            def bw_film(get_frame, t):
                frame = get_frame(t).astype('float64')
                # –ó–µ—Ä–Ω–∏—Å—Ç–æ—Å—Ç—å
                noise = np.random.normal(0, 18, frame.shape)
                frame = frame + noise
                # –ö–æ–Ω—Ç—Ä–∞—Å—Ç
                frame = (frame - 128) * 1.15 + 128
                # –í–∏–Ω—å–µ—Ç–∫–∞
                for c in range(3):
                    frame[:, :, c] *= vignette
                return np.clip(frame, 0, 255).astype('uint8')

            return bw_clip.fl(bw_film)

        # 2. SEPIA ‚Äî —Ç—ë–ø–ª—ã–π —Ç–æ–Ω + –≤–∏–Ω—å–µ—Ç–∫–∞
        elif filter_name == "Sepia":
            print("   -> Applying Sepia")
            w_v, h_v = clip.size
            vignette = self._make_vignette(h_v, w_v, 0.3)

            def sepia_effect(get_frame, t):
                frame = get_frame(t).astype(np.float64)
                sepia_matrix = np.array([
                    [0.393, 0.769, 0.189],
                    [0.349, 0.686, 0.168],
                    [0.272, 0.534, 0.131]
                ])
                sepia_frame = frame @ sepia_matrix.T
                # –¢—ë–ø–ª—ã–π –æ—Ç—Ç–µ–Ω–æ–∫
                sepia_frame[:, :, 0] *= 1.06  # –ö—Ä–∞—Å–Ω—ã–π +
                sepia_frame[:, :, 2] *= 0.88  # –°–∏–Ω–∏–π -
                # –í–∏–Ω—å–µ—Ç–∫–∞
                for c in range(3):
                    sepia_frame[:, :, c] *= vignette
                return np.clip(sepia_frame, 0, 255).astype('uint8')

            return clip.fl(sepia_effect)

        # 3. RAINBOW ‚Äî –±–æ–ª–µ–µ –ø–ª–∞–≤–Ω—ã–µ –ø–µ—Ä–µ–ª–∏–≤—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –∏—Å—Ö–æ–¥–Ω—ã–µ —Ü–≤–µ—Ç–∞
        elif filter_name == "Rainbow":
            print("   -> Applying Rainbow")
            def color_cycle(get_frame, t):
                frame = get_frame(t).astype(float)
                # –ü–ª–∞–≤–Ω–∞—è —Å–º–µ–Ω–∞ –æ—Ç—Ç–µ–Ω–∫–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º 70% –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
                r_shift = (np.sin(t * 2.5) + 1) / 2
                g_shift = (np.sin(t * 2.5 + 2.09) + 1) / 2
                b_shift = (np.sin(t * 2.5 + 4.19) + 1) / 2

                frame[:, :, 0] = frame[:, :, 0] * 0.7 + (r_shift * 200 * 0.3)
                frame[:, :, 1] = frame[:, :, 1] * 0.7 + (g_shift * 200 * 0.3)
                frame[:, :, 2] = frame[:, :, 2] * 0.7 + (b_shift * 200 * 0.3)

                return np.clip(frame, 0, 255).astype('uint8')

            return clip.fl(color_cycle)

        # 4. RUMBLE ‚Äî —Ç—Ä—è—Å–∫–∞ + –ª—ë–≥–∫–∏–π motion blur
        elif filter_name == "Rumble":
            print("   -> Applying Rumble")
            w, h = clip.size
            clip_zoomed = clip.resize(1.12)

            def rumble_effect(get_frame, t):
                dt = int(t * 20)
                random.seed(dt)
                dx = random.randint(-18, 18)
                dy = random.randint(-18, 18)

                cx = (clip_zoomed.w - w) / 2
                cy = (clip_zoomed.h - h) / 2

                frame = clip_zoomed.get_frame(t)[
                    int(cy + dy) : int(cy + dy + h),
                    int(cx + dx) : int(cx + dx + w)
                ]

                # Motion blur ‚Äî —Å–º–µ—à–∏–≤–∞–µ–º —Å —á—É—Ç—å —Å–¥–≤–∏–Ω—É—Ç—ã–º –∫–∞–¥—Ä–æ–º
                if abs(dx) > 8 or abs(dy) > 8:
                    blurred = cv2.GaussianBlur(frame, (5, 5), 0)
                    frame = cv2.addWeighted(frame, 0.7, blurred, 0.3, 0)

                return frame

            return clip.fl(rumble_effect)

        # 5. VHS ‚Äî —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–æ–º–µ—Ö–∏ + jitter + color bleed
        elif filter_name == "VHS":
            print("   -> Applying VHS")
            def vhs_effect(get_frame, t):
                frame = get_frame(t)
                h_img, w_img = frame.shape[:2]
                frame_float = frame.astype(float)

                # RGB Split (—Ö—Ä–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–±–µ—Ä—Ä–∞—Ü–∏—è) ‚Äî –ø–æ–¥–≤–∏–∂–Ω—ã–π —Å–¥–≤–∏–≥
                shift = 5 + int(2 * np.sin(t * 4))
                r_channel = np.roll(frame_float[:, :, 0], shift=shift, axis=1)
                g_channel = frame_float[:, :, 1]
                b_channel = np.roll(frame_float[:, :, 2], shift=-shift, axis=1)
                merged = np.stack([r_channel, g_channel, b_channel], axis=2)

                # Scanlines ‚Äî –∫–∞–∂–¥–∞—è 2-—è —Å—Ç—Ä–æ–∫–∞
                merged[::2, :] *= 0.88

                # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–π jitter ‚Äî —Å–ª—É—á–∞–π–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ–¥—ë—Ä–≥–∏–≤–∞—é—Ç—Å—è
                jitter_count = 4 + int(3 * abs(np.sin(t * 7)))
                jitter_rows = np.random.choice(h_img, size=jitter_count, replace=False)
                for row in jitter_rows:
                    merged[row] = np.roll(merged[row], np.random.randint(-10, 10), axis=0)

                # Tracking error (–±–µ–≥—É—â–∞—è –ø–æ–ª–æ—Å–∞ —à—É–º–∞)
                noise_y = int((t * 100) % h_img)
                noise_h = 15 + int(15 * abs(np.sin(t * 3)))
                end_y = min(noise_y + noise_h, h_img)
                if end_y > noise_y:
                    noise = np.random.randint(-70, 70, (end_y - noise_y, w_img, 3))
                    merged[noise_y:end_y, :] += noise

                # Color bleed (—Ä–∞–∑–º—ã—Ç–∏–µ —Ü–≤–µ—Ç–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–æ–≤)
                merged[:, :, 0] = cv2.GaussianBlur(merged[:, :, 0].astype('float32'), (5, 1), 0)
                merged[:, :, 2] = cv2.GaussianBlur(merged[:, :, 2].astype('float32'), (5, 1), 0)

                return np.clip(merged, 0, 255).astype('uint8')

            return clip.fx(vfx.lum_contrast, contrast=1.3).fl(vhs_effect)

        # 6. GROOVY ‚Äî —Ç—ë–ø–ª—ã–µ —Ç–æ–Ω–∞ + —à–ª–µ–π—Ñ + –ø–æ–∫–∞—á–∏–≤–∞–Ω–∏–µ
        elif filter_name == "Groovy":
            print("   -> Applying Groovy")
            clip_delayed = clip.fl_time(lambda t: max(0, t - 0.15), keep_duration=True)
            clip_blend = CompositeVideoClip([clip, clip_delayed.set_opacity(0.5)])

            w, h = clip.size
            clip_zoomed = clip_blend.resize(1.12)

            def groovy_pos(get_frame, t):
                # –ü–ª–∞–≤–Ω–æ–µ –ø–æ–∫–∞—á–∏–≤–∞–Ω–∏–µ —Å —Ç—ë–ø–ª—ã–º —Ç–æ–Ω–æ–º
                dx = int(np.sin(t * 2.5) * 25)
                dy = int(np.cos(t * 1.8) * 15)

                cx = (clip_zoomed.w - w) / 2
                cy = (clip_zoomed.h - h) / 2

                frame = clip_zoomed.get_frame(t)[
                    int(cy + dy) : int(cy + dy + h),
                    int(cx + dx) : int(cx + dx + w)
                ].astype(float)

                # –¢—ë–ø–ª—ã–π —Ü–≤–µ—Ç–æ–≤–æ–π —Å–¥–≤–∏–≥
                frame[:, :, 0] *= 1.08  # –ö—Ä–∞—Å–Ω—ã–π +
                frame[:, :, 1] *= 1.02  # –ó–µ–ª—ë–Ω—ã–π +
                frame[:, :, 2] *= 0.88  # –°–∏–Ω–∏–π -

                return np.clip(frame, 0, 255).astype('uint8')

            return clip_zoomed.fl(groovy_pos)

        print(f"   -> No matching filter found for '{filter_name}'")
        return clip

    def process_video(
        self,
        input_path: str,
        output_filename: str,
        trim_start: float = None,
        trim_end: float = None,
        crop: dict = None,
        remove_audio: bool = False,
        new_audio_path: str = None,
        text_config: dict = None,
        filter_name: str = None
    ) -> str:
        clip = None
        try:
            print(f"üé¨ START PROCESSING video: {input_path}")
            clip = VideoFileClip(input_path)
            
            # 1. Trimming
            if trim_start is not None and trim_end is not None:
                start = max(0, trim_start)
                end = min(clip.duration, trim_end)
                if start < end:
                    clip = clip.subclip(start, end)

            # 2. Cropping
            if crop:
                clip = clip.crop(
                    x1=crop.get('x', 0),
                    y1=crop.get('y', 0),
                    width=crop.get('width'),
                    height=crop.get('height')
                )

            # 3. Audio
            if remove_audio:
                clip = clip.without_audio()
            elif new_audio_path and os.path.exists(new_audio_path):
                new_audio = AudioFileClip(new_audio_path)
                if new_audio.duration > clip.duration:
                    new_audio = new_audio.subclip(0, clip.duration)
                clip = clip.set_audio(new_audio)

            # 4. Filters
            clip = self._apply_filter(clip, filter_name)

            # 5. Text
            if text_config and text_config.get('text'):
                # Size already scaled to actual video pixels by frontend
                fontsize = max(12, float(text_config.get('size', 50)))

                txt_clip = TextClip(
                    text_config['text'],
                    fontsize=fontsize,
                    color=text_config.get('color', 'white'),
                    font='DejaVu-Sans-Bold',
                    stroke_color='black',
                    stroke_width=max(1, int(fontsize / 25))
                )
                # Center text at the specified position (frontend uses translate(-50%,-50%))
                tx = float(text_config.get('x', 0.5)) * clip.w - txt_clip.w / 2
                ty = float(text_config.get('y', 0.8)) * clip.h - txt_clip.h / 2
                txt_clip = txt_clip.set_position((tx, ty)).set_duration(clip.duration)
                
                clip = CompositeVideoClip([clip, txt_clip])

            # Save
            output_path = os.path.join(self.output_dir, output_filename)
            clip.write_videofile(
                output_path,
                codec='libx264',
                audio_codec='aac',
                preset='ultrafast',
                fps=24,
                threads=4,
                logger='bar' # –í–∫–ª—é—á–∞–µ–º –ª–æ–≥–≥–µ—Ä moviepy
            )
            
            print(f"‚úÖ DONE: {output_path}")
            return output_path

        except Exception as e:
            print(f"‚ùå VideoEditorService Error: {e}")
            raise e
        finally:
            if clip:
                try: 
                    clip.close()
                except: 
                    pass